<!DOCTYPE HTML><html><head><title>RSS Team 13</title><link rel="stylesheet" type="text/css" href="../../css/style.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']],displayMath: [['$$','$$']],skipTags: ["script","noscript","style","textarea","code"]},TeX: {equationNumbers: {autoNumber: "AMS"}}});</script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script></head><body><div id="main"><div id="logo"><div id="logo_text"><h1><a href="../.."><span class="logo_colour">RSS Team 13</span></a></h1><h2>MIT Spring 2019</h2></div></div><div id="header"><div id="menubar"><ul id="menu"><li class=""><a href="../..">Home</a></li><li class="selected"><a href="../../labs">Labs</a></li><li><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">Video</a></li><li><a href="https://github.mit.edu/rss2019-13">Github</a></li></ul></div></div><div id="site_content"><div id="content"><h1>Lab 5: Localization</h1>
Our briefing slides can be found <a href="https://docs.google.com/presentation/d/e/2PACX-1vTX9UHYMN6F9P_uf9IQ6b1OAXbz3sXFKd_jL8gPUZn_0H1Jb4tpJtq0qgGNKi-lr2JAXZma9b8WczIM/embed?start=false&amp;loop=false&amp;delayms=3000">here</a>.
<hr>
<h2><strong>Overview and Motivations</strong> (Mia)</h2>
For lab 5, our goal was to give the robot the ability to find its position in a known environment. Localization is important for our future tasks in path planning and following. Without knowing where the racecar is, it is impossible to follow a path. We can do this by integrating the velocity of the robot as given by the interoceptive sensors measuring wheel motion. However as the car drives, error is builds up between this estimated position and the actual position, as a result of the surface it is driving on as well as wheel alignment and servo error. Therefore we must not solely rely on interoceptive sensors to localize. Instead we use a particle filter which takes input from both the interoceptive motion sensors, and LIDAR data. The particles in the particle each represent a guess for the robot’s position and heading. As the car drives, the motion model updates each particle based on the odometry. Then at a frequency of 8 Hz, the particle filter prunes the particles based on whether the LIDAR scan matches with the position, duplicating the points that were not pruned.
Using our implementation in the simulator, we were able to
<hr>
<h2>Table of Contents</h2>
Motion Model
Sensor Model
Particle Filter
SLAM?
<hr>
<h2><strong>Proposed Approach</strong></h2>
<h3><em>Motion Model</em> (Mia)</h3>
The motion model takes in an array of particles and updates them according to the odometry measurements from the robots internal sensors. The robot outputs an odometry message which gives an approximate position [x, y, ፀ] in a global frame. The motion model takes the difference between the current position and the previous position to get [dx, dy, dፀ] in the odometry frame.
<img src="https://drive.google.com/open?id=14jsAD1c1eZi8pzeezz6YRyr2v_RCTtpp" alt="Odometry Frame" height="474" width="573">
<br>Then the differences in position are rotated by the negative of its angle in odometry coordinates.
<img src="https://drive.google.com/open?id=1uKtovqCwPBCsbLXBirO6edtgLkD7kRoS" alt="Car Frame" height="462" width="583">
<br><img src="https://drive.google.com/open?id=1t7Kh0-cNQcf1TxZq201GpsRCQtQeNdw1" alt="Car Frame Rotation" height="100" width="496">
<br>Then noise is added to each particle randomly using a normal distribution.
<img src="https://drive.google.com/open?id=1G2fua__Wffb1HbkPeoUB7yR7yNL0-iJK" alt="Adding Noise" height="158" width="927">
<br>Then the differences in position are rotated to map coordinates according to each particle’s angle.
<img src="https://drive.google.com/open?id=10MG8khs0M42zed0mdMHaLdvmIYQTIwO0" alt="Map Frame" height="587" width="705">
<br><img src="https://drive.google.com/open?id=1YQ-_vXY8JTIcpgq0jOMs3MLTunQAqs5S" alt="Map Rotation" height="85" width="464">
<br>Then each change in position is added to its particle.
<br/><br/><h3><em>Sensor Model (Nada)</em></h3>
Once we had determined particle positions with the motion model, the sensor model used LIDAR data to filter particles based on probability. In this way, we were able to use our LIDAR sensor data and the robot’s current particle distribution, and compute the probability of receiving our LIDAR readings given the car location denoted by each particle in our motion model distribution. We then used these probabilities to update particle weights and determine the most likely car position.
We calculated each particle’s likelihood based on four factors: the probability of detecting a known obstacle in the map, the probability of a short measurement, the probability of a very large/missed measurement, and the probability of a random measurement.
These probabilities were defined as followed:
<img src="https://drive.google.com/uc?export=view&id=1BbGoKBRhd75FGShA9HBLQFdDmKtOmln7" alt="phit" height="100" width="400">
<br><strong>Figure 3A: Calculating p_hit</strong>
The probability of detecting a known obstacle in the map was represented as a gaussian distribution centered around the ground truth distance between the hypothesis pose and the nearest map obstacle.
<br/><br/><img src="https://drive.google.com/uc?export=view&id=1OcmCqA5pXyZKDzSe_bwgoy-HwejmLkWS" alt="pshort" height="100" width="400">
<br><strong>Figure 3B: Calculating p_short</strong>
The probability of a short measurement was represented as a downward sloping line as the ray gets further from the robot. This could happen if an object that was not accounted for in the map was detected by the robot before a wall was.
<br/><br/><img src="https://drive.google.com/uc?export=view&id=1N1YYRUOQmxf5M7G9-SabJ45citCjG8av" alt="pshort" height="100" width="400">
<br><strong>Figure 3C: Calculating p_max</strong>
The probability of a very large measurement was represented as a large spike in probability at the maximal range value. This would occur if the ground truth distance was larger than the maximum LIDAR reading distance.
<br/><br/><img src="https://drive.google.com/uc?export=view&id=10rHN6TO6L8_ED4lO6_n5HDSM0goDL5BL" alt="pshort" height="100" width="400">
<br><strong>Figure 3D: Calculating p_rand</strong>
The probability of a random measurement was represented by a small uniform value.
<br/><br/>From here, we mixed these four distributions by a weighted average as follows:
<img src="https://drive.google.com/uc?export=view&id=17wS-hThxEVmGtmoKbmubv5UmoW5vYJbA" alt="pshort" height="75" width="600">
<br><strong>Figure 4: Calculating p_total</strong>
To find the total probability of any given particle denoting the robot’s actual position, we simply added up all the individual probabilities, each multiplied by some factor such that the factors summed to one.
<br/><br/><br/><br/><br/><br/><h2><em>Precomputing the Sensor Model (Nada)</em></h2>
In order to speed up computation, we precomputed a discretized sensor model table that we could use to simply look up any values for z_t and z_t*. To do this, we computed all p_total values for all combinations of z_t and z_t* in the range of 0 to z_max, incrementing z_t and z_t* by 0.1 each time. In doing this, we were able to simply look up any probability given a z_t and z_t* value, which sped up our computation significantly.
<img src="https://drive.google.com/uc?export=view&id=15ePjn5CkA6SGagCsABDTC4qZRBxjKtNQ" alt="probgoal" height="100" width="400">
<br><strong>Figure 5: Probability Distribution of Precomputed Model</strong>
For any given ground truth distance (shown at the green line), a cross section like this one showed us the probability distribution of reading some measured distance from the LIDAR scan.
<br/><br/>Once we had these cross sections, we were able to create the entire lookup by creating the cross section for each given ground truth distance, and normalizing each distribution. Here you can see what this probability distribution looked like.
<br/><br/><img src="https://drive.google.com/uc?export=view&id=14FQ-v9YAzApMxqbjjSzSEOzVOu5Ja4Kr" alt="probdist" height="100" width="400">
<br><strong>Figure 6: Probability Distribution</strong>
This probability distribution shows all combinations of z_t and z_t*.
<br/><br/><br/><br/><h2><em>Applying the Sensor Model (Nada)</em></h2>
Once we had a precomputed lookup table, we could take in some particles from the motion model as well as the LIDAR observations, and use this data to find the likelihood of each motion model particle accurately denoting the robot’s position on the map. From here, we were able to choose the higher likelihood particles and update our pose estimate based on those particles.
<br/><br/><br/><br/><h2><strong>Experimental Evaluation</strong></h2>
<h3><em>In Simulation</em></h3>
<br/><br/><h3><em>On the Racecar</em></h3>
Though not as reliable as in simulation, the particle filter performs well on the car and is able to reliably update the car’s pose. It is much more difficult to measure error on the physical car than it is in the simulation, so the results in this section will largely be observations about the strengths and weaknesses of the algorithm when it is run on the car.
We did all of our testing in the basement of the Stata center and found that in hallway close to 32-044 produced very good results. The hallway has enough complicated features that there are rarely ambiguous positions, so the sensor model is able to update the car’s position accurately even as the motion model drifts away from the real result slightly. We found that it was important to tune the relative frequency of motion model and sensor model updates. The motion model is updated every odometry message, while the sensor model is only evaluated every five scans. If this ratio is too large the sensor model is not run frequently enough and the pose is allowed to drift with odometry error, but if it is run too frequently the car’s motion is typically ignored as the results are effectively overridden by the sensor model.
On hallways such as the wider rectangular spaces further from the classroom, the car can sometimes struggle as the sensor model can not uniquely identify its position along the hall.
<h2><strong>Lessons Learned (Nada)</strong></h2>
Although we attempted to keep our past lessons learned in mind while doing this lab, we definitely still came across similar problems for this lab. We learned that these labs always take longer than we expect, and although we attempted to front-load the work for this lab, we still worked right up to the deadline in order to get this lab working. In the future, we will continue to work ahead of schedule as much as possible to ensure that we are able to get everything done in time and satisfactorily.
We also learned during this lab that it is important to maintain good communication when working on these labs. It was difficult to meet for this lab due to spring break and other obligations, and making sure we all stayed in contact and communicated out schedules was crucial to us being able to work through this lab.
We also learned that the robot behaves very different in the simulator than in real life. Tuning the robot to work well in the simulator results in malfunction in real life and tuning the robot to work in real life did not perform well in the simulator.
<br/><br/><br/><br/></div></div></div></body></html>